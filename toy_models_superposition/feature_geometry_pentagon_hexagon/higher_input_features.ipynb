{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "import torch as t\n",
    "from torch import nn, Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "from typing import Optional, Callable, Union, List, Tuple\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chapter = \"chapter1_transformer_interp\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = (exercises_dir / \"part4_superposition_and_saes\").resolve()\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from plotly_utils import imshow, line, hist\n",
    "from part31_superposition_and_saes.utils import (\n",
    "    plot_features_in_2d,\n",
    "    plot_features_in_Nd,\n",
    "    plot_features_in_Nd_discrete,\n",
    "    plot_correlated_features,\n",
    "    plot_feature_geometry,\n",
    "    frac_active_line_plot,\n",
    ")\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not t.backends.mps.is_available():\n",
    "    if not t.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    device = t.device(\"mps\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_lr(step, steps):\n",
    "    return (1 - (step / steps))\n",
    "\n",
    "def constant_lr(*_):\n",
    "    return 1.0\n",
    "\n",
    "def cosine_decay_lr(step, steps):\n",
    "    return np.cos(0.5 * np.pi * step / (steps - 1))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # We optimize n_instances models in a single training loop to let us sweep over\n",
    "    # sparsity or importance curves  efficiently. You should treat `n_instances` as\n",
    "    # kinda like a batch dimension, but one which is built into our training setup.\n",
    "    n_instances: int\n",
    "    n_features: int = 5\n",
    "    n_hidden: int = 2\n",
    "    n_correlated_pairs: int = 0\n",
    "    n_anticorrelated_pairs: int = 0\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    W: Float[Tensor, \"n_instances n_hidden n_features\"]\n",
    "    b_final: Float[Tensor, \"n_instances n_features\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        feature_probability: Optional[Union[float, Tensor]] = None,\n",
    "        importance: Optional[Union[float, Tensor]] = None,\n",
    "        device = device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if feature_probability is None: feature_probability = t.ones(())\n",
    "        if isinstance(feature_probability, float): feature_probability = t.tensor(feature_probability)\n",
    "        self.feature_probability = feature_probability.to(device)\n",
    "        if importance is None: importance = t.ones(())\n",
    "        if isinstance(importance, float): importance = t.tensor(importance)\n",
    "        self.importance = importance.to(device).broadcast_to((cfg.n_instances, cfg.n_features))\n",
    "\n",
    "        self.W = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_hidden, cfg.n_features))))\n",
    "        self.b_final = nn.Parameter(t.zeros((cfg.n_instances, cfg.n_features)))\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: Float[Tensor, \"... instances features\"]\n",
    "    ) -> Float[Tensor, \"... instances features\"]:\n",
    "\n",
    "        h = einops.einsum(features, self.W, \"... instances features, instances n_hidden features -> ... instances n_hidden\")\n",
    "        \n",
    "        output = einops.einsum(h, self.W, \"... instances n_hidden, instances n_hidden features -> ... instances features\")\n",
    "        \n",
    "        return F.relu(output + self.b_final).to(device)\n",
    "\n",
    "\n",
    "    def generate_batch(self, batch_size) -> Float[Tensor, \"batch_size instances features\"]:\n",
    "        '''\n",
    "        Generates a batch of data. We'll return to this function later when we apply correlations.\n",
    "        '''\n",
    "        features = t.rand((batch_size, self.cfg.n_instances, self.cfg.n_features), device = self.W.device)\n",
    "    \n",
    "        feature_seed = t.rand((batch_size, self.cfg.n_instances, self.cfg.n_features), device = self.W.device)\n",
    "        \n",
    "        feat_is_present = feature_seed <= self.feature_probability\n",
    "        \n",
    "        batch = t.where(feat_is_present, features, 0.0)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        out: Float[Tensor, \"batch instances features\"],\n",
    "        batch: Float[Tensor, \"batch instances features\"],\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        '''\n",
    "        Calculates the loss for a given batch, using this loss described in the Toy Models paper:\n",
    "\n",
    "            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n",
    "\n",
    "        Remember, `model.importance` will always have shape (n_instances, n_features).\n",
    "        '''\n",
    "        squared_loss = self.importance * (batch - out) ** 2\n",
    "    \n",
    "        loss = einops.reduce(squared_loss, \"batch instances features -> instances\", \"mean\")\n",
    "        \n",
    "        summed_loss = loss.sum()\n",
    "        \n",
    "        return summed_loss, loss\n",
    "\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        batch_size: int = 1024,\n",
    "        steps: int = 10_000,\n",
    "        log_freq: int = 100,\n",
    "        lr: float = 1e-3,\n",
    "        lr_scale: Callable[[int, int], float] = constant_lr,\n",
    "    ):\n",
    "        '''\n",
    "        Optimizes the model using the given hyperparameters.\n",
    "        '''\n",
    "        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n",
    "\n",
    "        progress_bar = tqdm(range(steps))\n",
    "        \n",
    "        losses = []\n",
    "        summed_losses = []\n",
    "\n",
    "        for step in progress_bar:\n",
    "\n",
    "            # Update learning rate\n",
    "            step_lr = lr * lr_scale(step, steps)\n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] = step_lr\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            batch = self.generate_batch(batch_size)\n",
    "            out = self(batch)\n",
    "            summed_loss, loss = self.calculate_loss(out, batch)\n",
    "            losses.append(loss.detach().cpu().clone().numpy().tolist())\n",
    "            summed_losses.append(summed_loss.detach().cpu().clone().item())\n",
    "            summed_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Display progress bar\n",
    "            if step % log_freq == 0 or (step + 1 == steps):\n",
    "                progress_bar.set_postfix(loss=summed_loss.item()/self.cfg.n_instances, lr=step_lr)\n",
    "                \n",
    "        return summed_losses,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_probability(n_instances):\n",
    "    feature_prob = (50 ** -t.linspace(0, 1, n_instances))\n",
    "    feature_prob = einops.rearrange(feature_prob, \"instances -> instances ()\")\n",
    "    return feature_prob\n",
    "\n",
    "def feature_importance(n_features):\n",
    "    importance = (0.7 ** t.arange(n_features))\n",
    "    importance = einops.rearrange(importance, \"features -> () features\")\n",
    "    return importance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    n_instances = 8,\n",
    "    n_features = 8,\n",
    "    n_hidden = 2,\n",
    ")\n",
    "\n",
    "# importance varies within features for each instance\n",
    "importance = feature_importance(cfg.n_features)\n",
    "\n",
    "# sparsity is the same for all features in a given instance, but varies over instances\n",
    "feature_prob = feature_probability(cfg.n_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    importance = importance,\n",
    "    feature_probability = feature_prob,\n",
    ")\n",
    "summed_losses, losses = model.optimize(steps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(losses, columns=[str(np.round(prob.item(),3)) for prob in feature_prob])\n",
    "loss_df[\"total_loss\"] = loss_df.apply(lambda x: x.sum(),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = np.log(loss_df).plot(title = f\"Loss Curves: {cfg.n_features} features represented in 2D space\", height=600).update_layout(xaxis_title=\"Steps\", yaxis_title=\"Log loss\")\n",
    "fig.write_image(\"higher_input_feature_plots/random_test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(loss_df).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df.columns.name = 'Sparsity'\n",
    "np.log(loss_df).plot(title = f\"Loss Curves: {cfg.n_features} features represented in 2D space\", height=600).update_layout(\n",
    "    xaxis_title=\"Steps\", yaxis_title=\"Log loss\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(t.corrcoef(model.W.detach()[-1].T).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W.detach()[-1].T, t.corrcoef(model.W.detach()[-1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_in_2d(\n",
    "    model.W.detach(),\n",
    "    colors = model.importance,\n",
    "    title = f\"Superposition: {cfg.n_features} features represented in 2D space\",\n",
    "    subplot_titles = [f\"1 - S = {i:.3f}\" for i in feature_prob.squeeze()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_feature_plots(n_feature):\n",
    "    print(f\"n_feature:{n_feature}\")\n",
    "\n",
    "    cfg = Config(\n",
    "        n_instances = 8,\n",
    "        n_features = n_feature,\n",
    "        n_hidden = 2,\n",
    "    )\n",
    "    \n",
    "    # importance varies within features for each instance\n",
    "    importance = feature_importance(cfg.n_features)\n",
    "    \n",
    "    # sparsity is the same for all features in a given instance, but varies over instances\n",
    "    feature_prob = feature_probability(cfg.n_instances)\n",
    "\n",
    "    model = Model(\n",
    "        cfg = cfg,\n",
    "        device = device,\n",
    "        importance = importance,\n",
    "        feature_probability = feature_prob,\n",
    "    )\n",
    "    summed_losses, losses = model.optimize(steps=10_000)\n",
    "\n",
    "    loss_df = pd.DataFrame(losses, columns=[np.round(prob.item(),3) for prob in feature_prob])\n",
    "    loss_df[\"total_loss\"] = loss_df.apply(lambda x: x.sum(),axis = 1)\n",
    "    \n",
    "    loss_df.columns.name = 'Sparsity'\n",
    "    \n",
    "    np.log(loss_df).plot(title = f\"Loss Curves: {cfg.n_features} features represented in 2D space\", height=600).update_layout(xaxis_title=\"Steps\", yaxis_title=\"Log loss\").show()\n",
    "\n",
    "    loss_df.plot(title = f\"Loss Curves: {cfg.n_features} features represented in 2D space\", height=600).update_layout(xaxis_title=\"Steps\", yaxis_title=\"Loss\").show()\n",
    "\n",
    "    plot_features_in_2d(\n",
    "        model.W.detach(),\n",
    "        colors = model.importance,\n",
    "        title = f\"Superposition: {cfg.n_features} features represented in 2D space\",\n",
    "        subplot_titles = [f\"1 - S = {i:.3f}\" for i in feature_prob.squeeze()],\n",
    "    )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_input_features_runs(n_features):\n",
    "    print(f\"n_features:{n_features}\")\n",
    "\n",
    "    cfg = Config(\n",
    "            n_instances = 8,\n",
    "            n_features = n_features,\n",
    "            n_hidden = 2,\n",
    "        )\n",
    "\n",
    "    \n",
    "    # importance varies within features for each instance\n",
    "    importance = feature_importance(cfg.n_features)\n",
    "    \n",
    "    # sparsity is the same for all features in a given instance, but varies over instances\n",
    "    feature_prob = feature_probability(cfg.n_instances)\n",
    "\n",
    "    model = Model(\n",
    "        cfg = cfg,\n",
    "        device = device,\n",
    "        importance = importance,\n",
    "        feature_probability = feature_prob,\n",
    "    )\n",
    "    summed_losses, losses = model.optimize(steps=10_000)\n",
    "\n",
    "    results_dict = {}\n",
    "    results_dict[\"W\"] = model.W.detach()\n",
    "    results_dict[\"b\"] = model.b_final.detach()\n",
    "    results_dict[\"summed_loss\"] = summed_losses\n",
    "    results_dict[\"losses\"] = losses\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_list = list(range(2,11))\n",
    "results_list = []\n",
    "\n",
    "for n_feature in n_features_list:\n",
    "    results_list.append(different_input_features_runs(n_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_frob_norm(W):\n",
    "    return np.mean([t.norm(w).item() for w in W])\n",
    "\n",
    "def max_frob_norm(W):\n",
    "    return np.max([t.norm(w).item() for w in W])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(n_features_list)):\n",
    "    print(f\"n_feature: {n_features_list[i]}\")\n",
    "    W = results_list[i][\"W\"].cpu()\n",
    "    print(f\"avg norm: {avg_frob_norm(W)}\")\n",
    "    print(f\"max norm: {max_frob_norm(W)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature_list = list(range(2,11))\n",
    "input_feature_plots(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_plots(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_plots(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_plots(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_plots(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_plots(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_plots(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_plots(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_plots(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bf1b3ab3bf4993279d6e4d507e32e29da12e88795dbb4a207694a5b99d95980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
