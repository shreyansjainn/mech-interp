{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a717407-206e-4606-93df-6529aad92b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "import random\n",
    "import torch as t\n",
    "from torch import nn, Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "from typing import Optional, Callable, Union, List, Tuple\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import copy\n",
    "import shap\n",
    "import plotly.express as px\n",
    "\n",
    "chapter = \"chapter1_transformer_interp\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = (exercises_dir / \"part4_superposition_and_saes\").resolve()\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from plotly_utils import imshow, line, hist\n",
    "from part31_superposition_and_saes.utils import (\n",
    "    plot_features_in_2d,\n",
    "    plot_features_in_Nd,\n",
    "    plot_features_in_Nd_discrete,\n",
    "    plot_correlated_features,\n",
    "    plot_feature_geometry,\n",
    "    frac_active_line_plot,\n",
    "    animate_features_in_2d\n",
    ")\n",
    "\n",
    "from feature_geometry_utils import *\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not t.backends.mps.is_available():\n",
    "    if not t.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    device = t.device(\"mps\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb18fe48-18f5-4327-b485-dbcb8ce16e19",
   "metadata": {},
   "source": [
    "t.manual_seed(20)\n",
    "\n",
    "importance = (1  ^  t.arange(cfg.n_features))\n",
    "\n",
    "feature_probability = (25  ^  -t.linspace(0, 1, cfg.n_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c447a8-4b49-414d-92fc-cd4aa13cfb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_probability(n_instances):\n",
    "    feature_prob = (25 ** -t.linspace(0, 1, n_instances))\n",
    "    feature_prob = einops.rearrange(feature_prob, \"instances -> instances ()\")\n",
    "    return feature_prob\n",
    "\n",
    "def feature_importance(n_features):\n",
    "    importance = (1 ** t.arange(n_features))\n",
    "    importance = einops.rearrange(importance, \"features -> () features\")\n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf9078-06b1-4afe-9bc0-3beeb870109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_probability(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc49ede-3471-45f1-868b-a01e928f5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb58b4-4659-44f7-a530-f6dab1ffa796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiment_metadata(n_features,seed, n_instances = 8,optim_fn=t.optim.Adam):\n",
    "    print(f\"n_features:{n_features}\")\n",
    "    print(f\"seed:{seed}\")\n",
    "\n",
    "    cfg = Config(\n",
    "            n_instances = n_instances,\n",
    "            n_features = n_features,\n",
    "            n_hidden = 2,\n",
    "            optim_fn = optim_fn\n",
    "        )\n",
    "\n",
    "    \n",
    "    # importance varies within features for each instance\n",
    "    importance = feature_importance(cfg.n_features)\n",
    "    \n",
    "    # sparsity is the same for all features in a given instance, but varies over instances\n",
    "    feature_prob = feature_probability(cfg.n_instances)\n",
    "\n",
    "    model = Model(\n",
    "        cfg = cfg,\n",
    "        device = device,\n",
    "        importance = importance,\n",
    "        feature_probability = feature_prob,\n",
    "    )\n",
    "    summed_losses, losses, batches,all_W, all_b, per_feature_losses, test_losses, test_summed_losses, test_per_feature_losses = model.optimize(steps=10_000)\n",
    "\n",
    "    results_dict = {}\n",
    "    results_dict[\"n_features\"] = n_features\n",
    "    results_dict[\"seed\"] = seed\n",
    "    results_dict[\"model\"] = model\n",
    "    results_dict[\"importance\"] = model.importance\n",
    "    results_dict[\"feature_prob\"] = feature_prob\n",
    "    results_dict[\"W\"] = model.W.detach()\n",
    "    results_dict[\"b\"] = model.b_final.detach()\n",
    "    results_dict[\"summed_loss\"] = summed_losses\n",
    "    results_dict[\"per_feature_losses\"] = per_feature_losses\n",
    "    results_dict[\"losses\"] = losses\n",
    "    results_dict[\"test_summed_losses\"] = test_summed_losses\n",
    "    results_dict[\"test_per_feature_losses\"] = test_per_feature_losses\n",
    "    results_dict[\"test_losses\"] = test_losses\n",
    "    results_dict[\"batches\"] = batches\n",
    "    results_dict[\"all_W\"] = all_W\n",
    "    results_dict[\"all_b\"] = all_b\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5df87d-282f-4852-95f0-bb7245014a22",
   "metadata": {},
   "source": [
    "## Notes 11th Jan\n",
    "- Case 1: where 6th feature is not learned (pentagon representation)\n",
    "- Case 2: where 6th feature is learned and feature norm is high (hexagon representation)\n",
    "- Case 3: where 6th feature is learned and feature norm is low (pentagon representation)\n",
    "\n",
    "\n",
    "## To Do:\n",
    "- save the random seed and initialization for future runs\n",
    "- isolate that particular seed and sparsity combination for reproducibility to check how much initialization is affecting hexagon (train just 1 instance)\n",
    "- is there a propensity of learning 5 vs 6 feature based on initialization\n",
    "- what the optimizer is doing in all the cases and how its affecting the no of features learned\n",
    "- what if we slightly change the initialization, does it change it much or changes it a lot (can denote where in the peak that point is)\n",
    "- how much the optimizer is throwing it away or towards from an initialization\n",
    "- \n",
    "\n",
    "## Bigger question\n",
    "- why 5 and not any other no of features like 6 or 7\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36781bd5-30b3-4f05-a935-584ab8e73af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_list = random.sample(range(1, 100), 8)\n",
    "random_seed_list.extend([20,26])\n",
    "print(random_seed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef46055-b5ee-42b1-b080-a10e18fdfcb7",
   "metadata": {},
   "source": [
    "#### Hexagon seeds: 26, 20, 88, 87\n",
    "\n",
    "\n",
    "- Case 1: where 6th feature is not learned (pentagon representation)(seed=26 & sparsity=0.057)\n",
    "- Case 2: where 6th feature is learned and feature norm is high (hexagon representation) (seed=20, sparsity=0.057 & seed=26, sparsity=0.040)\n",
    "- Case 3: where 6th feature is learned and feature norm is low (pentagon representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f123a4-f617-435b-b20b-0f27d7ca937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_hexagon_replication = []\n",
    "\n",
    "for seed in random_seed_list:\n",
    "    t.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    results_list_hexagon_replication.append(save_experiment_metadata(6,seed,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da8ba4-f4bb-4d27-b8dc-ec4813190610",
   "metadata": {},
   "outputs": [],
   "source": [
    "for results in results_list_hexagon_replication:\n",
    "    print(\"n_features:\",results['n_features'])\n",
    "\n",
    "    plot_features_in_2d(\n",
    "        results['W'],\n",
    "        colors = results['importance'],\n",
    "        title = f\"Superposition: {results['n_features']} features represented in 2D space\",\n",
    "        subplot_titles = [f\"1 - S = {i:.3f}\" for i in results['feature_prob'].squeeze()],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c92fa-f15e-4f54-980e-269ae6fc499b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "results_list_heptagon_replication = []\n",
    "\n",
    "for n in range(10):\n",
    "    results_list_heptagon_replication.append(save_experiment_metadata(7,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03df9a-efbe-4477-800a-b9b0bf1e5b29",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "for results in results_list_heptagon_replication:\n",
    "    print(\"n_features:\",results['n_features'])\n",
    "\n",
    "    plot_features_in_2d(\n",
    "        results['W'],\n",
    "        colors = results['importance'],\n",
    "        title = f\"Superposition: {results['n_features']} features represented in 2D space\",\n",
    "        subplot_titles = [f\"1 - S = {i:.3f}\" for i in results['feature_prob'].squeeze()],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f19fcf0-352a-4faa-895b-396f78bcd55b",
   "metadata": {},
   "source": [
    "### TO DO:\n",
    "- analyze both the 6 feature instances in 6 features and see what the norm, test loss, per feature loss trends are in it \n",
    "- compare it to the instance where 5 and 1 overlap is present\n",
    "- analyze the 6 feature instance in 7 features, and compare it to that of 6 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ad216-30e5-466b-a797-eca9b73485ea",
   "metadata": {},
   "source": [
    "# Analyzing Hexagon in n_features = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d76e4-9e14-4cdf-b27d-0dc021ec88b5",
   "metadata": {},
   "source": [
    "## 1st Instance, 2nd iteration, second lowest sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833c5f1-3958-43f6-a943-07a57474d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_hexagon_nfeatures_6 = results_list_hexagon_replication[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bef0de-8c9d-4d37-a5e4-fe967cfa5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_hexagon_nfeatures_6.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e96fc-08fe-4027-80f9-3002cf8b17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(first_hexagon_nfeatures_6['losses'])[index].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dac689-87a7-40cc-b895-fd4a6e9e4324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c39b65-131d-4c03-a63d-b187b131c358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6abba96-dfed-4f63-be75-0fbf7c04b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vs_feature_learned(result, index):\n",
    "    print(\"n_feature:\", result['n_features'])\n",
    "    feature_wise_norm = [t.norm(t.tensor(W[index]),dim=0).cpu().tolist() for W in result['all_W']]\n",
    "    overall_norm = [t.norm(t.tensor(W[index])).item() for W in result['all_W']]\n",
    "    n_features_learned = [sum(np.round(np.abs(W[index]).sum(axis = 0),0)>0).item() for W in result['all_W']]\n",
    "    df = pd.DataFrame(feature_wise_norm)\n",
    "    df['overall_norm'] = overall_norm\n",
    "    \n",
    "    fig = df.plot()\n",
    "        \n",
    "    # Add the second trace for y2\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df.index, y=n_features_learned, name='n_features_learned', yaxis='y2')\n",
    "    )\n",
    "    \n",
    "    # Update layout for dual axes\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(title='norm/n_features_learned', side='left'),\n",
    "        yaxis2=dict(title='loss', overlaying='y', side='right'),\n",
    "        xaxis=dict(title='x'),\n",
    "        title='Dual Axis Plot with Plotly'\n",
    "    )\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "def per_feature_loss_viz(result, index, test_loss=False, log=False,moving_avg_window = 20):\n",
    "    print(\"n_feature:\", result['n_features'])\n",
    "    if not test_loss:\n",
    "        per_feature_loss_key = 'per_feature_losses'\n",
    "        loss_key = 'losses'\n",
    "        title='Per Feature loss vs overall loss'\n",
    "    else:\n",
    "        per_feature_loss_key = 'test_per_feature_losses'\n",
    "        loss_key = 'test_losses'\n",
    "        title = \"Test Per Feature loss vs test overall loss\"\n",
    "        \n",
    "    per_feature_loss_df = pd.DataFrame([per_feature_loss[index] for per_feature_loss in result[per_feature_loss_key]])\n",
    "    per_feature_loss_df['overall_loss'] = pd.DataFrame(result[loss_key])[index].values\n",
    "    n_features_learned = [sum(np.round(np.abs(W[index]).sum(axis = 0),0)>0).item() for W in result['all_W']]\n",
    "\n",
    "    moving_avg_loss = per_feature_loss_df.rolling(window=moving_avg_window).mean()\n",
    "    \n",
    "    if not log:\n",
    "        fig = moving_avg_loss.iloc[moving_avg_window-1:].plot()    \n",
    "    else:\n",
    "        fig = np.log(moving_avg_loss.iloc[moving_avg_window-1:]).plot()\n",
    "        \n",
    "    # Add the second trace for y2\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=moving_avg_loss.index[moving_avg_window-1:], y=n_features_learned[moving_avg_window-1:], name='n_features_learned', yaxis='y2')\n",
    "    )\n",
    "    \n",
    "    # Update layout for dual axes\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(title='per_feature_loss/overall loss', side='left'),\n",
    "        yaxis2=dict(title='n_features_learned', overlaying='y', side='right'),\n",
    "        xaxis=dict(title='x'),\n",
    "        title=title\n",
    "    )\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3358e1-3014-4484-a0ea-d27ea08f6cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e6e31-7a59-4a80-973b-f68b157deb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_feature_loss_df.rolling(window=moving_avg_window).mean().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376abc79-18b0-4594-a614-1b0f164e7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vs_feature_learned(results_list_hexagon_replication[5],9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc34f5-a6f6-4475-9bf7-561f5476ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_feature_loss_viz(results_list_hexagon_replication[0], 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691cea6-9225-41d5-a039-84469ebb6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vs_feature_learned(first_hexagon_nfeatures_6,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0e46d-7952-4182-b9b5-c98de8f18e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vs_feature_learned(first_hexagon_nfeatures_6,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd132aa-0d41-4e92-a887-ccb1a63faaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_feature_loss_viz(first_hexagon_nfeatures_6, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54062e77-14a7-4674-b747-d2dd0721c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_feature_loss_viz(first_hexagon_nfeatures_6, 8,test_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce130f4-a232-4f44-8f4b-b1ee875d787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vs_feature_learned(first_hexagon_nfeatures_6,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6763d-44c9-4135-b929-5eb11ea7d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_feature_loss_viz(first_hexagon_nfeatures_6, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf741eee-f88c-4a72-b57d-f3670c6137c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_feature_loss_viz(first_hexagon_nfeatures_6, 9,test_loss = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1e157249407422164c408460dd402e3c6b799474849211cb1b647dec9fcf706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
